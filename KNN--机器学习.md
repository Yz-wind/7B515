# K-近邻算法(KNN)笔记

## 1、什么是k-近邻算法

### 1.1、KNN概念

K Nearest Neighbor算法又叫KNN算法。

定义：如果一个样本在特征空间中的K个最相似（即特征空间中最近邻）的样本中的大多数属于某一个类别，则该样本也属于这个类别。

### 1.2距离公式

#### 1.2.1欧氏距离

两个样本的距离可以通过如下公式计算，又叫**欧式距离**

![image-20231107153145125](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20231107153145125.png)

（1）二维平面上点a($x_1$,$y_1$)与b($x_2$,$y_1$)间的欧氏距离：

​	$d_{12}$=$\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$

（2）三维空间点$(x_1,y_1,z_1)$与$(x_2,y_2,z_2)$间的欧式距离：

​	$d_{12}$=$\sqrt{(x_1-x_2)^2+(y_1-y_2)^2+(z_1-z_2)^2}$

（3）n维空间点$(x_{11},x_{12},...,x_{1n})$与$(X_{21},x_{22},...,x_{2n}$间的欧氏距离（两个n维向量）：

​	$d_{12}$=$\sqrt{\sum_{k=1}^n(x_{1k}-x_{2k})^2}$

#### 1.2.2曼哈顿距离(Manhattan Distance)

定义：在曼哈顿街区要从一个十字路口开车到另一个十字路口，驾驶距离显然不是两点间的直线距离。这个实际驾驶距离就是“曼哈顿距离”。曼哈顿距离也称为“城市街区距离”

![image-20231107170854986](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20231107170854986.png)

（1）二维平面两点$(x_1,y_1)$与$(x_2,y_2)$间的曼哈顿距离：

​	$d_{12}$=$|x_1-x_2|+|y_1-y_2|$

（2）n维空间点$a(x_{11},x_{12},...,x_{1n})$与$b(x_{21},x_{22},...,x_{2n})$的曼哈混距离：

​	$d_{12}$=$\sum_{k=1}^n{|x_{1k}-x_{2k}|}$

#### 1.2.3切比雪夫距离(Chebyshev Distance)

定义：国际象棋中，国王可以直行、横行、斜行，所有国王走一步可以移动到相邻8个方格中的任意一个，国王从格子$(x_1,y_1)$走到格子$(x_2,y_2)$最少需要多少步？这个距离就叫切比雪夫距离。

![image-20231107172313361](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20231107172313361.png)

（1）二维平面两点$a(x_1,y_1)$与$b(x_2,y_2)$间的切比雪夫距离：

​	$d_{12}$=$\max(|x_1-x_2|,|y_1-y_2|)$

（2）n维空间点$a(x_{11},x_{12},...,x_{1n})$与$b(x_{21},b_{22},...,b_{2n})$的切比雪夫距离：

​	$d_{12}$=$\max(|x_{1i}-x_{2i}|)$

#### 1.2.4闵可夫斯基距离(Minkowski Distance)

定义：闵氏距离不是一种距离，是对多个距离度量公式的概括性的表述。

两个n维变量$a(x_{11},x_{12},...,x_{1n})$与$b(x_{21},x_{22},...,x_{2n})$间的闵可夫斯距离定义为：

​	**$d_{12}$=$\sqrt[p]{\sum_{k=1}^n{|x_{1k}-x_{2k}|^p}}$**

其中p是一个变参数：

（1）当p=1时，就是曼哈顿距离；

（2）当p=2时，就是欧氏距离；

（3）当$p\to\infty$时，就是切比雪夫距离。

根据p的不同，闵氏距离可以表示某一类/种距离。

**闵氏距离的缺点**：

​		（1）将各个分量的量纲(scale)，也就是“单位”相同看待了；

​		（2）未考虑各个分量的分布(期望，方差等)可能是不同的。

#### 1.2.5解决上诉问题的方法

##### 1.2.5.1标准化欧式距离(Standardized EuclideanDistance)

标准化欧氏距离是针对欧氏距离的缺点而作的一种改进。

思路：既然数据各维分量的分布不一样，那先将各个分量都“标准化”到均值、方差相等。假设样本集X的均值(mean)为**m**，标准差(standard deviation)为**s**，**X的“标准化变量”表示为**：

​				$X^*$=$(X-m)/s$

**标准化欧氏距离公式**：

​				$d_{12}$=$\sqrt{\sum_{k=1}^n{((x_{1k}-x_{2k})/s_k})^2}$       ,其中$s_k$为标准差

补充：（1）均值：$\overline{X}$=$(\sum_{i=1}^n{X_I})/n$

​			（2）标准差：$s$=$\sqrt{(\sum_{i=1}^n{X_i-\overline{X}})/(n-1)}$

##### 1.2.5.2余弦距离(Cosine Distance)

几何种，夹角余弦可用来衡量两个方向的差异；机器学习中，借用这一概念来衡量样本向量之间的差异。

（1）二维空间中向量$A(x_1,y_1)$与向量$B(x_2,y_2)$的夹角余弦公式：

​		$\cos(\Theta)$=$(x_1*x_2+y_1*y_2)/(\sqrt{x_1^2+y_1^2}+\sqrt{x_2^2+y_2^2})$

（2）两个n维样本点$a(x_{11},x_{12},...,x_{1n})$和$b(x_{21},x_{22},...,x_{2n})$的夹角余弦为：

​		$\cos(\Theta)$=$(\sum_{k=1}^n{x_{1k}*x_{2k}})/(\sqrt{\sum_{k=1}^n{x_{1k}^2}}*\sqrt{\sum_{k=1}^n{x_{2k}^2}})$

夹角余弦取值范围为[-1，1].余弦越大表示两个向量的夹角越小，余弦越小表示两向量的夹角越大。当两个想i昂的方向重合时，余弦去最大值1；当两个向量的方向完全相反时，余弦去最小值-1。

### 1.3k值选择

（1）**k**值过小：容易受到异常点的影响

（2）**k**值过大：受到样本均衡的问题

**近似误差**：对现有训练集的训练误差，关注训练集，如果近似误差过小可能会出现过拟合的现象，对现有的训练集能有很好的预测，但是对未知的测试样本将会出现较大的偏差的预测，模型本身不是最接近最佳模型。

**估计误差**：对测试集的测试误差，关注测试集，估计误差小说明对未知数据的预测能力好，模型本身最接近最佳模型。

#### 1.3.1k值选择问题

​	（一）选择较小的k值，就相当于用较小的领域中的训练实例进行训练，“学习”近似误差会减小，只有与输入实例较劲或相似的训练实例菜会对预测结果起作用。**k值的减小就意味着整体模型变得复杂，容易发生过拟合。**

​	（二）选择较大的k值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少嘘唏的估计误差，但缺点是学习的近似误差会增大。这时，**与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且k值的增大就意味着整体模型变得简单。**

​	（三）k=N（N为训练样本个数），则完全不足取，因此此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单忽略了训练实例中大量有用的信息。

###  1.4kd树

为了提高KNN搜索的效率，可以考虑使用特殊的结构存储训练数据，以减小机算距离的次数。

根据KNN每次需要预测一个点时，我们都需要机算训练数据集里每个点到这个点的距离。然后选出距离最近的k个点进行投票。当数据集很大时，这个机算成本非常高，针对N个样本，D个特征的数据集，其算法复杂度为O(DN^2)。

#### 1.4.1定义

是**一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。**kd树是一种二叉树，表示对k维空间的一个划分，**构造kd树相当于不断地用垂直于坐标抽的超平面讲k维空间切分，构成一系列的k维超矩形区域。**kd树的每个借点对应于一个k维超矩形区域。**利用kd树可以省去部分数据点的搜索，从而减少搜索的计算量。**

#### 1.4.2 构造方法

（1）构造根结点，使根结点对应于k维空间中包含所有实例点的超矩形区域；

（2）通过递归的方法，不断地对k维空间进行切分，生成子结点。在超矩形区域上选择一个坐标抽和在词坐标抽上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标抽，将当前超矩形区域切分为左右两个子区域（子结点）；这时，实例被分到两个子区域。

（3）上述过程直到子区域内没有实例时终止（终止时的结点为叶结点）。在此过程中，讲实例保存在相应的结点上。

（4）通常，循环的选择坐标抽时对空间切分，选择训练实例点在坐标抽的中位数为切分点，这样得到的kd树是平衡的（平衡二叉树：它是一棵空树，或其左子树和右子树的深度之差的绝对值不超过1，且它的左子和右子树都是平衡二叉树）。

kd树种每个节点是一个向量，和二叉树按照数的大小划分不同的是，kd树每层需要选定向量中的某一维，然后根据这一维按左小右大的方式划分数据。在构建kd树时，关键需要解决2个问题：

（1）选择向量的哪一维进行划分；

（2）如何划分数据；

第一个问题简单的解决方法是可以随机选择某一维或按顺序选择，但更好的方法应该是在数据比较分散的那一维进行划分（**分散的程度可以根据方差来衡量**）。好的划分方法可以使构建的树比较平衡，可以每次选择中位数来进行划分，这样问题2也得到了解决。

ps：方差公式

​	$S^2$=$(\sum_{i=1}^N（{X_i-\overline{X}})^2)/N$

#### 1.4.3举例

给定一个二维空间数据集：T={(2,3),(5,4),(9,6),(4,7),(8,1),(7,2)}，构造一个平衡kd树。

![image-20231109163551078](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20231109163551078.png)

![image-20231109163659415](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20231109163659415.png)

第一次：

x轴--2,5,9,4,8,7-->2,4,5,7,8,9（x轴方差比较大）

y轴--3,4,6,7,1,2-->1,2,3,4,6,7

首先选择x轴，找中间点（本次构造树选择的根结点是（7，2））

第二次：

左面：(2,3),(4,7),(5,4)-->y轴(经排序后)：3，4，7

右面：(8,1),(9,6)-->y轴（经排序后）：1，6

从y轴开始选择，左边选择点是（5，4），右边选择点是（9，6）

第三次，从x轴开始选择，过程同上

#### 1.4.4最近领域的搜索

举例1：以（2.1，3.1）为圆心，r=0.141

![image-20231109165356328](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20231109165356328.png)

举例2：以（2，4.5）为圆心，r=3.202

![image-20231109170307886](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20231109170307886.png)
